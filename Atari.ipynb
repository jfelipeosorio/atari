{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Atari.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanosoriodata/atari/blob/master/Atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0dmSa_c8wnG",
        "colab_type": "text"
      },
      "source": [
        "Hecho por *Juan Felipe Osorio Ramírez* en el curso de Minería de Datos en la Universidad Nacional de Colombia, Sede Bogotá.\n",
        "\n",
        "El contenido (imágenes y teoría) se toma de las siguientes referencias y tiene con último fin la academia:\n",
        "1. Saito, S., Wenzhuo, Y., & Shanmugamani, R. (2018). Python Reinforcement Learning Projects: Eight hands-on projects exploring reinforcement learning algorithms using TensorFlow. Packt Publishing Ltd.\n",
        "\n",
        "2. Evans, L. C. (1983). An introduction to mathematical optimal control theory version 0.2. Lecture notes available at http://math.berkeley.edu/~evans/control.course.pdf .\n",
        "\n",
        "3. Sutton, R. S., & Barto, A. G. (1998). Introduction to reinforcement learning (Vol. 135). Cambridge: MIT press."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZM-whkq6_-l",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "# ATARI\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_kC6KA09AXH",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "23ebb53b-63cd-4708-efb8-9706f58036d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "#@title\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "Image(url= \"data:image/png;base64,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\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"data:image/png;base64,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\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "Este cuaderno reproduce un proyecto de Aprendizaje Reforzado cuyo alcance es:\n",
        "- Implementar un emulador del juego Atari usando `gym`.\n",
        "- Explorar el preprocesamiento de datos en tareas de este tipo de aprendizaje, en este caso para el juego Atari.\n",
        "- Explorar el algoritmo Q-learning.\n",
        "- Implementar DQN usando TensorFlow y visualizar el entrenamiento.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYQiEOHxAVAK",
        "colab_type": "text"
      },
      "source": [
        "## Intuición y Matemáticas detrás de escenas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4vQR2qc140M",
        "colab_type": "text"
      },
      "source": [
        "### ¿Qué es el Aprendizaje Reforzado?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmq6FAGrxuUe",
        "colab_type": "text"
      },
      "source": [
        "En este tipo de machine learning, se tiene un **agente** que se refiere al algoritmo/modelo que aprende una tarea específica.\n",
        "\n",
        "El agente aprende principalmente al recibir una **señal de recompensa**, que es un escalar que indica que tan bien el agente está realizando la tarea.\n",
        "\n",
        "Suponga que tenemos un agente cuya tarea es controlar el movimiento de caminar de un robot. El agente recibirá una recompensa positiva si logra hacer caminar el robot al destino fijado y recompensa negativa si se cae o no camina en dirección del destino.\n",
        "\n",
        "Además, estas señales de recompensa se retornan como consecuencia de una serie de **acciones** que el agente realiza; esto a diferencia de como se retribuye la señal en un modelo de aprendizaje supervisado, en el cual se hace inmediatamente se entrena el modelo. Las acciones son simplemente esas opciones disponibles que tiene el agente para hacer en el **ambiente**. El ambiente se refiere al mundo en el que el agente vive y se encarga principalmente de retornar las señales de recompensa al agente. Las acciones del agente normalmente están condicionadas a lo que el agente percibe del ambiente. Lo que el agente percibe se concibe como la **observación** o el **estado** del ambiente. \n",
        "\n",
        "Nota: Lo que distingue el aprendizaje reforzado de otros paradigmas es que las acciones que realiza el agente pueden alterar el ambiente y sus respuestas subsecuentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tkjbBvZ2O1C",
        "colab_type": "text"
      },
      "source": [
        "#### Ejemplo: Space Invaders de Atari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9HMIoQD2Ut_",
        "colab_type": "text"
      },
      "source": [
        "En este caso el agente tiene como tarea jugar **Space Invaders**, el juego de Arcade Atari 2600. El ambiente es el juego como tal y la lógica que este tiene en sus especificaciones. Durante el juego, el agente consulta al ambiente para hacer una observación (que es equivalente a tener un estado). Aquí la observación es un array de tamaño $(210,160,3)$, que coincide  con la pantalla del juego que muestra la nave del agente, los enemigos, el punntaje y cualquier proyectil. Basado en esta observación, el agente realiza algunas acciones, que pueden incluir el moverse a la izquierda o derecha, disparar un laser o quedarse quieto. El ambiente recibe la acción del agente como *input* y hace los respectivos *updates* al estado.\n",
        "\n",
        "Por ejemplo, si el laser toca una nave enemiga, se remueve del juego. Si el agente decide moverse a la izquierda, el juego hará el update de cambiar las coordenadas de acuerdo a esto. Este proceso se repite hasta que se alcanza **estado terminal**, un estado que representa el final de una sucesión. En Space Invaders, el estado terminal se alcanza cuando la nave del agente es destruida, y el juego subsecuentemente retorna el puntaje que lleva, que se calcula con base en el número de naves enemigas que se destruyeron.\n",
        "\n",
        "Nota: Algunos ambientes no tienen estados terminales, como el mercado de acciones. Estos siguen andando mientras existan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsfJ7tC95ueQ",
        "colab_type": "text"
      },
      "source": [
        "#### Resumen de términos  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRVC7Irm53kl",
        "colab_type": "text"
      },
      "source": [
        "Término | Descripción | Ejemplos\n",
        "--- | --- | ---\n",
        "Agente | Un algoritmo/modelo que aprende una tarea específica. | Carros Self-Driving, Robots que caminan, **jugador de video juegos**. \n",
        "Ambiente | El mundo en el que el agente actúa. Es responsable de controlar los que el agente percibe y provee la retroalimentación sobre que tan bien el agente realiza la tarea. | El camino sobre el que anda el carro, **un video juego**, el mercado de las acciones.\n",
        "Acción | La decisión que toma el agente en un ambiente, usualmente depende de lo que el agente percibe | Dirigir un carro, comprar o vender una acción, **disparar un laser de la nave que el agente controla**.\n",
        "Señal de recompensa| Un escalar que indica que tan bien el agente está realizando una tarea.| **El puntaje de Space Invaders**, ROI de una compra de acción, distancia recorrida por un robot que intenta caminar.\n",
        "Observación/Estado| Una descripción de el ambiente tal como la percibe el agente | Video desde una cámara, **la pantalla del juego**, estadísticas del mercado de acciones.\n",
        "Estado terminal|Un estado en el que el agente no puede realizar más acciones.| Llegar al final de un laberinto, **la nave del agente en Space Invaders es destruida**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1WhjnhSEwS_",
        "colab_type": "text"
      },
      "source": [
        "Formalmente, miremos que sucede en el tiempo $t$ con un agente $P$ y un ambiente $E$:\n",
        "1. $P$ consulta una observación $s_t$ de $E$. \n",
        "2. $P$ decide tomar una acción $a_t$ basada en la observación $s_t$.  \n",
        "3. $E$ recibe $a_t$ y retorna una recompensa $r_t$ basado en la acción.\n",
        "4. $P$ recibe $r_t$.\n",
        "5. $E$ actualiza $s_t$ a $s_{t+1}$  basado en $a_t$  y otros factores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ5jG0mYH6rT",
        "colab_type": "text"
      },
      "source": [
        "Ahora, ¿De qué manera el ambiente calcula $r_t$ y $s_{t+1}$?\n",
        "\n",
        "Para esto el ambiente usualmente tiene su propio algoritmo que calcula estos valores basado en numerosos *inputs/factors*, incluyendo la acción que el agente realiza.\n",
        "A continuación, vamos a discutir en mayor detalle el mayor protagonista de cada problema de aprendizaje reforzado- el agente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u4oCx-kLSvI",
        "colab_type": "text"
      },
      "source": [
        "#### El Agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1aqzaYcLxIb",
        "colab_type": "text"
      },
      "source": [
        "El objetivo del agente de aprendizaje reforzado es aprender a desempeñar una tarea bien en un ambiente. Matemáticamente, esto significa que queremos maximizar la recompensa acumulada $R$, que se expresa así:\n",
        "\n",
        "$R= r_0+\\gamma^1r_1+\\cdots+\\gamma^tr_t$\n",
        "\n",
        "En donde simplemente calculamos una suma ponderada de la recompensa recibida en cada tiempo $t$. $\\gamma$ se llama un **factor de descuento**, y es un escalar que toma valores entre $0$ y $1$. La idea detrás de este factor es tal que a mayor tiempo transcurrido tiene la recompensa, menor valor tiene en R. Esto refleja también la perspectiva en las recompensas del mundo real, en la que preferimos recibir $100.000$ COP ahora *vs.* en un año. Ya que el valor de la recompensa puede ser mayor cuando estamos más cerca del presente. \n",
        "\n",
        "Debido a que la mecánica del ambiente no es completamente observable para el agente, este último debe ganar información al tomar acciones y observar como el ambiente reacciona a estas. Y de esta manera los humanos aprender a desempeñar sus tareas a lo largo de la vida.\n",
        "\n",
        "Supongamos que estamos aprendiendo a jugar ajedrez. Si bien no tenemos todos los movimiento posibles guardados en memoria o no sabemos como jugará nuestro oponente, podemos mejorar nuestra competencia con el tiempo. En particular, podemos llegar a ser competentes en lo siguiente: \n",
        "\n",
        "- Aprender como reaccionar a un movimiento que hace nuestro oponente.\n",
        "- Evaluar que tan buena es nuestra posición para ganar el juego.\n",
        "- Predecir que hará nuestro oponente en la siguiente jugada y usar esto para decidir nuestro movimienot actual.\n",
        "- Entender como jugarían los jugadores en general en una situación en la que nos encontramos en el momento.\n",
        "\n",
        "De hecho, los agentes de aprendizaje reforzado pueden aprender a hacer cosas similares. En particular, un agente está compuesto de múltiples funciones y modelos para asistir su toma de decisión. Existen tres componentes que los agentes pueden tener: \n",
        "- Una **política**\n",
        "- Una **función de valor**\n",
        "- Un **modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjvLYZEISzHN",
        "colab_type": "text"
      },
      "source": [
        "#### La *política* del *agente*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYZ4piW-TYbT",
        "colab_type": "text"
      },
      "source": [
        "Una **política** es un algoritmo o un conjunto de reglas que describe como un agente toma sus decisiones. Un ejemplo de una, puede ser la estrategia que un inversionista usa para invertir en sus acciones, donde este compra una acción cuando los precios bajan y la vende cuando los precios suben.\n",
        "\n",
        "Formalmente, una política es una función, denotada por $\\pi$ que mapea un estado $s_t$ a una acción $a_t$:\n",
        "\n",
        "$\\pi(s_t)=a_t$\n",
        "\n",
        "Lo cuál significa que el agente decide su acción tomando en cuenta su estado actual. Esta función puede representar muchas cosas, en la medida en que reciba un estado como input y una acción como output, sea una tabla, un grafo, o un clasificador de machine learning.\n",
        "\n",
        "Siendo estrictos en la manera que definimos la política, esta resulta ser **determinística** y no en todas las aplicaciones es así. Es por esto que aparece una manera más general de definir la política y es de manera **estocástica** donde esta tiene como output una distribución de probabilidad sobre el conjunto de acciones posibles en un estado:\n",
        "\n",
        "$\\pi(a_t|s_t)=P(a_t|s_t)$\n",
        "\n",
        "donde $\\pi(a_t| s_t)$ es un vector de probabilidades normalizado sobre el conjunto de posibles acciones dado un estado. Aplicando al ejemplo de Atari este concepto de política, veamos la siguiente imagen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3oLIem3lDQO",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://learning.oreilly.com/library/view/python-reinforcement-learning/9781788991612/assets/0ce55cbe-0718-4ad8-a8f7-f45d19212236.png)\n",
        "\n",
        "Imagen: Como la política mapea el estado del juego (la pantalla) a las acciones (probabilidades)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5aYMI9IS-uB",
        "colab_type": "text"
      },
      "source": [
        "#### La *función de valor* del agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlRjH-jBmC0l",
        "colab_type": "text"
      },
      "source": [
        "El segundo componente que un agente puede tener es la **función de valor**. Como se mencionó en la introducción, es útil evaluar la posición, si es buena o mala, en un estado dado. En una partida de ajedrez, a un jugador le gustaría saber la verosimilitud de que va a ganar dado un estado del tablero. Un agente navegando un laberinto le gustaría saber que tan cerca está de llegar a su destino. La idea es que la función de valor quiere satisfacer los propósitos anteriores, ya que *predice el valor esperado de recompensa que un agente recibiría en un estado dado*. En otras palabras, mide que tan deseable es para el agente, su estado actual.\n",
        "\n",
        "Formalmente, la función de valor toma un estado y una política como input y retorna un valor escalar representando la esperanza de la recompensa acumulada:\n",
        "\n",
        "$v(s,\\pi)= E[R|s,\\pi]=E[r_0+\\gamma^1r_1+\\cdots+\\gamma^tr_t|s,\\pi]$\n",
        "\n",
        "Ahora, ¿De qué manera esta función ayuda al agente a desempeñar una tarea bien, a parte de informar sobre que tan deseable es estar en un estado dado? Como veremos luego, las funciones de valor juegan un papel importante en predecir que tan bueno será el tomar una serie de acciones antes de que sean tomadas por el agente. Esto es similar a que los jugadores de ajedrez se imaginen qué tan bien una secuencia de acciones futuras funcionará para mejorar sus posibilidades de ganar. Para esto, el agente también necesita tener un entendimiento de como el ambiente opera. Y este entendimiento, resulta en el tercer componente de un agente que es, **el modelo**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UspD7gneTECy",
        "colab_type": "text"
      },
      "source": [
        "#### El *modelo* del agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmdifE3wsBRV",
        "colab_type": "text"
      },
      "source": [
        "Anteriormente, se discutió como el agente no conoce completamente el ambiente. En otras palabras, el agente usualmente no tiene una idea de como se ve internamente el algoritmo completo del ambiente.\n",
        "\n",
        "El agente necesita interactuar con el ambiente para ganar información y aprender como maximizar el valor esperado de la recompensar acumulada. Sin embargo, es posible para el agente tener una réplica interna, o un modelo, de el ambiente. El agente puede usar el modelo para predecir de qué manera el ambiente reaccionaría a alguna acción en un estado dado. Un modelo en el mercado de las acciones, por ejemplo, tiene la tarea de predecir como los precios serán en un futuro. Si el modelo es preciso, el agente puede entonces usar su función de valor para evaluar que tan deseable es el futuro. \n",
        "\n",
        "Formalmente, el modelo puede ser representado por una función $M$ que calcula la probabilidad del siguiente estado dado el estado actual y un acción: (MONTENEGRO?)\n",
        "\n",
        "$M(a_t,s_t)=P(s_{t+1}|s_t,a_t)$\n",
        "\n",
        "En otro escenarios, el modelo del ambiente puede ser usado para enumerar posibles estados futuros. Esto es común en juegos basados por turno, tales como el ajedrez o triki, donde las reglas y alcance de las posibles acciones están claramente definidas. Normalmente se hacen árboles de para ilustrar estas posibles sucesiones de acciones y estados de juegos basados en turno:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZGxlsTVytR1",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://learning.oreilly.com/library/view/python-reinforcement-learning/9781788991612/assets/873f8b59-9dd2-4fc6-830a-9c5fd62198f9.png)\n",
        "\n",
        "Imagen: Un modelo usando su valor de función para evaluar sus posibles movimientos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMiWAmtx0Vxw",
        "colab_type": "text"
      },
      "source": [
        "En el ejemplo anterior del juego triki, $M(s_t,a_t)$ denota los posibles estados que al tomar una acción $a_t$ (representada en la imagen como un círculo azul con rayas dentro) podría tomar en un estado dado $s_t$. Más aún, podemos calcular el valor de cada estado usando el valor de la *función de valor*. Los estados intermedios o de abajo en la imagen llevarían a obtener un valor alto ya que el agente estaría a un paso de ganar, caso contrario al caso de arriba ya que aquí se debe preocupar de que el oponente gane.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XeeF2w-4ruc",
        "colab_type": "text"
      },
      "source": [
        "#### Resumen de términos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFkry3Xl41K7",
        "colab_type": "text"
      },
      "source": [
        "Término | Descripción | Output\n",
        "--- | --- | ---\n",
        "Política | Algoritmo/función que retorna decisiones que el agente toma. | Un escalar único (decisión) (política determinística) o un vector de probabilidades sobre posibles acciones (política estocástica)\n",
        "Función de Valor |  Función que describe que tan bueno o malo es un estado dado. | Un valor escalar que representa el valor esperado de la recompensa acumulada.\n",
        "Modelo | Una representación del ambiente que tiene el agente, que predice como reaccionará el ambiente a las acciones que el agente realiza.| La probabilidad del próximo estado dados una acción y el estado actual, o una enumeración de posibles estados dadas las reglas del ambiente.\n",
        "\n",
        "Finalmente, usaremos estos conceptos para aprender uno de los marcos conceptuales fundamentales en aprendizaje reforzado: los **Procesos de Decisión de Markov**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYGfJ6m6AJHh",
        "colab_type": "text"
      },
      "source": [
        "#### Procesos de Decisión de Markov (PDM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAsXm35YA5f7",
        "colab_type": "text"
      },
      "source": [
        "Un **proceso de decisión de Markov** es un marco usado para representar el **ambiente** en un problema de aprendizaje reforzado. Este se puede representar gráficamente mediante un grafo con arcos dirigidos. Cada nodo representa un posible estado en el ambiente, y cada arco apuntando a un estado representa la acción que se puede tomar en el estado actual. \n",
        "\n",
        ">*Formalizaremos el problema del aprendizaje reforzado usando ideas de la Teoría de Sistemas Dinámicos, específicamente, el **control optimal de un proceso de decisión de Markov parcialmente-conocido**.*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy7G5BvlcAPY",
        "colab_type": "text"
      },
      "source": [
        "Consideremos el siguiente ejemplo:\n",
        "\n",
        "![alt text](https://learning.oreilly.com/library/view/python-reinforcement-learning/9781788991612/assets/3689b1c6-d71e-44ff-b5bf-e7ddf2ac33aa.png)\n",
        "\n",
        "Imagen: Un Proceso de Decisión de Markov."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-b6ES_ZclHs",
        "colab_type": "text"
      },
      "source": [
        "El anterior PDM representa una jornada típica de un programador en un día. Cada círuclo representa un estado particular en el cual el programador puede estar, y en particular el estado azul (Wake up) es estado inicial (o el estado en $t=0$ que está el agente), y el estado naranja (Publish Code) denota el estado terminal. Cada estado tiene una recompensa que está asociada, y entre más alta sea, más deseable será.\n",
        "\n",
        "Además una representación del grafo del ejemplo es su **matriz de adyacencia**:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxNNibNqf4H7",
        "colab_type": "text"
      },
      "source": [
        "State/Action | Wake Up | Netflix | Code and Debug | Nap | Deploy | Sleep\n",
        "--- | --- | --- | --- | --- | --- | ---\n",
        "Wake Up | N/A | -2 | -3 | 0 | N/A | N/A\n",
        "Netflix | N/A | -2 | N/A | N/A | N/A | N/A\n",
        "Code and Debug | N/A | N/A | N/A | 1 | 10 | 3\n",
        "Nap | 0 | N/A | N/A | N/A | N/A | N/A\n",
        "Deploy | N/A | N/A | N/A | N/A | N/A | 3\n",
        "Sleep | N/A | N/A | N/A | N/A | N/A | N/A\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imIPes9wCVYK",
        "colab_type": "text"
      },
      "source": [
        "La columna de la izquierda representa los posibles estados y la primera fila representan las posibles acciones. N/A siginifa que la acción no se puede realizar estan en el estado dado. Este sistema básicamente representa las decisiones que un programador puede realizar en un día.\n",
        "\n",
        "Cuando el programador se levanta (*Wake up*), este puede decidir ir a trabajar (*Code and debug the code*) o ir a ver películas (*Netflix*). Nótese que la recompensa por ver Netflix es más alta que ir a trabajar. Para el programador en cuestión, ver Netflix resulta ser una actividad mejor recompensada, mientras que hacer código y debugging de este (que se espera no sea la misma idea del lector). Sin embargo, ambas acciones nos llevan a obtener recompensas negativas, pero recordemos que nuestro objetivo es maximizar la recompensa acumulada. Si el programador decide ver Netflix, se quedaría atrapado en un bucle infinito, que a la final termina debilitando la recompensa acumulada. Por otro lado, más estados con mejor recompensa estarán disponibles para el programador si se decide por trabajar.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R171E7y0IYMS",
        "colab_type": "text"
      },
      "source": [
        "Examinemos las posibles trayectorias, que son sucesiones de acciones que el programador puede tomar:\n",
        "- Wake Up | Netflix | Netflix | ...\n",
        "- Wake Up | Code and Debug | Nap | Wake Up | Code and Debug | Nap | ...\n",
        "- Wake Up | Code and Debug | Sleep\n",
        "- Wake Up | Code and Debug | Deploy | Sleep\n",
        "\n",
        "La primera y segunda trayectoria representan bucles infinitos. Calculemos la recompensa acumulada para cada una, donde hacemos $\\gamma = 0.9$:\n",
        "- $R = 0+0.9\\times(-2)+0.9^2\\times(-2)+0.9^3\\times(-2)\\cdots<0$\n",
        "- $R = 0+0.9\\times(-3)+0.9^2\\times(1)+0.9^3\\times(0)+0.9^4\\times(-3)+0.9^5\\times(1)\\cdots<0 $\n",
        "- $R = 0+0.9\\times(-3)+0.9^2\\times(3)=-0.27$\n",
        "- $R = 0+0.9\\times(-3)+0.9^2\\times(10)+0.9^3\\times(3)=7.587$\n",
        "\n",
        "Podemos ver que para la primera y segunda trayectoria, a pesar de no tener un estado terminal, nunca tendremos recompensas positivas. La cuarta trayectoria nos brinda la recompensa acumulada más alta.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VESiC1W2NIra",
        "colab_type": "text"
      },
      "source": [
        "Lo que hemos calculado entonces son las funciones de valor para cuatros políticas que un programador puede tomar en un día. Recordemos que la función de valor es la esperanza de la recompensa acumulada empezando por un estado dado y siguiente un política. \n",
        "\n",
        "Hemos observado cuatro posibles políticas y evaluado como cada una de ellas nos lleva a obtener una recompensa acumulada distinta, ejercicio conocido como **evaluación de política**.\n",
        "\n",
        "Más aún, las ecuaciones que hemos usado para calcular las recompensas esperadas son conocidas como **Ecuaciones de esperanza de Bellman**. Las ecuaciones de Bellman forman un conjunto de ecuaciones usadas para evaluar y mejorar políticas y funciones de valor para finalmente ayudar a un agente a desempeñar de mejor manera una tarea. \n",
        "\n",
        "Para ver más sobre estas ecuaciones fundamentales en el estudio de aprendizaje reforzado ver la referencia [3], al inicio de este cuaderno.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSX1lIH8PeNw",
        "colab_type": "text"
      },
      "source": [
        "#### ¿Qué sigue?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g6BwRi_PjWE",
        "colab_type": "text"
      },
      "source": [
        "Ahora que hemos pasado por términos y conceptos clave de aprendizaje reforzado, el lector se preguntará por cómo enseñar a un agente de aprendizaje reforzado a maximizar su recompensa, o en otras palabras, tomando el ejemplo del PDM del programador, a que sepa que la cuarta trayectoria es la mejor. \n",
        "\n",
        "Para nuestro proyecto con ATARI trabajaremos en la solución a este interrogante usando **Aprendizaje Profundo**. Para lo cuál haremos una sección introduciendo el tema. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRRYttZuRCvs",
        "colab_type": "text"
      },
      "source": [
        "### ¿Qué es el Aprendizaje Profundo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfYRibxwAi--",
        "colab_type": "text"
      },
      "source": [
        "## Infrastructura y Dependecias "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xq7h5UYnz5f",
        "colab_type": "text"
      },
      "source": [
        "### Instalamos OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrDrN5e4niSQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "eb590320-0211-4baf-95a8-90156eab128d"
      },
      "source": [
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 1s (655 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 144542 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,266 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.4 [784 kB]\n",
            "Fetched 784 kB in 1s (942 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 146897 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCP7h53wol3J",
        "colab_type": "text"
      },
      "source": [
        "### Instalamos el ambiente Atari de gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EtLZuDUohbq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "2704f7c7-1b7c-4ab6-92fd-cb7e4c08fa08"
      },
      "source": [
        "!pip install gym[atari]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.18.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GARAfL5qR5s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "1f87af25-aa64-4792-e174-b53c25ea8942"
      },
      "source": [
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "!pip install xdpyinfo"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.2.10)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: piglet-templates in /usr/local/lib/python3.6/dist-packages (from piglet) (0.5.1)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.3)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (19.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.34.2)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.12.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement xdpyinfo (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for xdpyinfo\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewVEnDZoqe7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9b568be8-c7d5-4b7e-be49-de7f763421ad"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOoaha9yqtT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrRuBopPqxFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwxUxrdZq1OT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeMwzBGmpthh",
        "colab_type": "text"
      },
      "source": [
        "### Hagamos unos tests en gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kSLUJd1pypJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16b280b1-dcab-46d6-e7ff-70977f1d367b"
      },
      "source": [
        "import gym\n",
        "environment = gym.make('CartPole-v0') \n",
        "environment.reset()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0145538 , -0.0192197 , -0.03278938,  0.01242186])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S0W8ek8Ax_5",
        "colab_type": "text"
      },
      "source": [
        "## Aprendizaje de Máquina en Acción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R37MbKayBAqC",
        "colab_type": "text"
      },
      "source": [
        "## "
      ]
    }
  ]
}