{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Atari.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanosoriodata/atari/blob/master/Atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0dmSa_c8wnG",
        "colab_type": "text"
      },
      "source": [
        "Hecho por *Juan Felipe Osorio Ramírez* en el curso de Minería de Datos en la Universidad Nacional de Colombia, Sede Bogotá.\n",
        "\n",
        "El contenido se toma de las siguientes referencias:\n",
        "1. Saito, S., Wenzhuo, Y., & Shanmugamani, R. (2018). Python Reinforcement Learning Projects: Eight hands-on projects exploring reinforcement learning algorithms using TensorFlow. Packt Publishing Ltd.\n",
        "\n",
        "2. Evans, L. C. (1983). An introduction to mathematical optimal control theory version 0.2. Lecture notes available at http://math. berkeley. edu/~ evans/control. course. pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZM-whkq6_-l",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "# ATARI\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_kC6KA09AXH",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "23ebb53b-63cd-4708-efb8-9706f58036d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "#@title\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "Image(url= \"data:image/png;base64,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\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"data:image/png;base64,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\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "Este cuaderno reproduce un proyecto de Aprendizaje Reforzado cuyo alcance es:\n",
        "- Implementar un emulador del juego Atari usando `gym`.\n",
        "- Explorar el preprocesamiento de datos en tareas de este tipo de aprendizaje, en este caso para el juego Atari.\n",
        "- Explorar el algoritmo Q-learning.\n",
        "- Implementar DQN usando TensorFlow y visualizar el entrenamiento.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYQiEOHxAVAK",
        "colab_type": "text"
      },
      "source": [
        "### Intuición y Matemáticas detrás de escenas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4vQR2qc140M",
        "colab_type": "text"
      },
      "source": [
        "### ¿Qué es el Aprendizaje Reforzado?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmq6FAGrxuUe",
        "colab_type": "text"
      },
      "source": [
        "En este tipo de machine learning, se tiene un **agente** que se refiere al algoritmo/modelo que aprende una tarea específica.\n",
        "\n",
        "El agente aprende principalmente al recibir una **señal de recompensa**, que es un escalar que indica que tan bien el agente está realizando la tarea.\n",
        "\n",
        "Suponga que tenemos un agente cuya tarea es controlar el movimiento de caminar de un robot. El agente recibirá una recompensa positiva si logra hacer caminar el robot al destino fijado y recompensa negativa si se cae o no camina en dirección del destino.\n",
        "\n",
        "Además, estas señales de recompensa se retornan como consecuencia de una serie de **acciones** que el agente realiza; esto a diferencia de como se retribuye la señal en un modelo de aprendizaje supervisado, en el cual se hace inmediatamente se entrena el modelo. Las acciones son simplemente esas opciones disponibles que tiene el agente para hacer en el **ambiente**. El ambiente se refiere al mundo en el que el agente vive y se encarga principalmente de retornar las señales de recompensa al agente. Las acciones del agente normalmente están condicionadas a lo que el agente percibe del ambiente. Lo que el agente percibe se concibe como la **observación** o el **estado** del ambiente. \n",
        "\n",
        "Nota: Lo que distingue el aprendizaje reforzado de otros paradigmas es que las acciones que realiza el agente pueden alterar el ambiente y sus respuestas subsecuentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tkjbBvZ2O1C",
        "colab_type": "text"
      },
      "source": [
        "#### Ejemplo: Space Invaders de Atari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9HMIoQD2Ut_",
        "colab_type": "text"
      },
      "source": [
        "En este caso el agente tiene como tarea jugar **Space Invaders**, el juego de Arcade Atari 2600. El ambiente es el juego como tal y la lógica que este tiene en sus especificaciones. Durante el juego, el agente consulta al ambiente para hacer una observación (que es equivalente a tener un estado). Aquí la observación es un array de tamaño $(210,160,3)$, que coincide  con la pantalla del juego que muestra la nave del agente, los enemigos, el punntaje y cualquier proyectil. Basado en esta observación, el agente realiza algunas acciones, que pueden incluir el moverse a la izquierda o derecha, disparar un laser o quedarse quieto. El ambiente recibe la acción del agente como *input* y hace los respectivos *updates* al estado.\n",
        "\n",
        "Por ejemplo, si el laser toca una nave enemiga, se remueve del juego. Si el agente decide moverse a la izquierda, el juego hará el update de cambiar las coordenadas de acuerdo a esto. Este proceso se repite hasta que se alcanza **estado terminal**, un estado que representa el final de una sucesión. En Space Invaders, el estado terminal se alcanza cuando la nave del agente es destruida, y el juego subsecuentemente retorna el puntaje que lleva, que se calcula con base en el número de naves enemigas que se destruyeron.\n",
        "\n",
        "Nota: Algunos ambientes no tienen estados terminales, como el mercado de acciones. Estos siguen andando mientras existan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsfJ7tC95ueQ",
        "colab_type": "text"
      },
      "source": [
        "#### Resumen de términos  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRVC7Irm53kl",
        "colab_type": "text"
      },
      "source": [
        "Término | Descripción | Ejemplos\n",
        "--- | --- | ---\n",
        "Agente | Un algoritmo/modelo que aprende una tarea específica. | Carros Self-Driving, Robots que caminan, **jugador de video juegos**. \n",
        "Ambiente | El mundo en el que el agente actúa. Es responsable de controlar los que el agente percibe y provee la retroalimentación sobre que tan bien el agente realiza la tarea. | El camino sobre el que anda el carro, **un video juego**, el mercado de las acciones.\n",
        "Acción | La decisión que toma el agente en un ambiente, usualmente depende de lo que el agente percibe | Dirigir un carro, comprar o vender una acción, **disparar un laser de la nave que el agente controla**.\n",
        "Señal de recompensa| Un escalar que indica que tan bien el agente está realizando una tarea.| **El puntaje de Space Invaders**, ROI de una compra de acción, distancia recorrida por un robot que intenta caminar.\n",
        "Observación/Estado| Una descripción de el ambiente tal como la percibe el agente | Video desde una cámara, **la pantalla del juego**, estadísticas del mercado de acciones.\n",
        "Estado terminal|Un estado en el que el agente no puede realizar más acciones.| Llegar al final de un laberinto, **la nave del agente en Space Invaders es destruida**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1WhjnhSEwS_",
        "colab_type": "text"
      },
      "source": [
        "Formalmente, miremos que sucede en el tiempo $t$ con un agente $P$ y un ambiente $E$:\n",
        "1. $P$ consulta una observación $s_t$ de $E$. \n",
        "2. $P$ decide tomar una acción $a_t$ basada en la observación $s_t$.  \n",
        "3. $E$ recibe $a_t$ y retorna una recompensa $r_t$ basado en la acción.\n",
        "4. $P$ recibe $r_t$.\n",
        "5. $E$ actualiza $s_t$ a $s_{t+1}$  basado en $a_t$  y otros factores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ5jG0mYH6rT",
        "colab_type": "text"
      },
      "source": [
        "Ahora, ¿De qué manera el ambiente calcula $r_t$ y $s_{t+1}$?\n",
        "\n",
        "Para esto el ambiente usualmente tiene su propio algoritmo que calcula estos valores basado en numerosos *inputs/factors*, incluyendo la acción que el agente realiza.\n",
        "A continuación, vamos a discutir en mayor detalle el mayor protagonista de cada problema de aprendizaje reforzado- el agente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u4oCx-kLSvI",
        "colab_type": "text"
      },
      "source": [
        "#### El Agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1aqzaYcLxIb",
        "colab_type": "text"
      },
      "source": [
        "El objetivo del agente de aprendizaje reforzado es aprender a desempeñar una tarea bien en un ambiente. Matemáticamente, esto significa que queremos maximizar la recompensa acumulada $R$, que se expresa así:\n",
        "\n",
        "$R= r_0+\\gamma^1r_1+\\cdots+\\gamma^tr_t$\n",
        "\n",
        "En donde simplemente calculamos una suma ponderada de la recompensa recibida en cada tiempo $t$. $\\gamma$ se llama un **factor de descuento**, y es un escalar que toma valores entre $0$ y $1$. La idea detrás de este factor es tal que a mayor tiempo transcurrido tiene la recompensa, menor valor tiene en R. Esto refleja también la perspectiva en las recompensas del mundo real, en la que preferimos recibir $100.000$ COP ahora *vs.* en un año. Ya que el valor de la recompensa puede ser mayor cuando estamos más cerca del presente. \n",
        "\n",
        "Debido a que la mecánica del ambiente no es completamente observable para el agente, este último debe ganar información al tomar acciones y observar como el ambiente reacciona a estas. Y de esta manera los humanos aprender a desempeñar sus tareas a lo largo de la vida.\n",
        "\n",
        "Supongamos que estamos aprendiendo a jugar ajedrez. Si bien no tenemos todos los movimiento posibles guardados en memoria o no sabemos como jugará nuestro oponente, podemos mejorar nuestra competencia con el tiempo. En particular, podemos llegar a ser competentes en lo siguiente: \n",
        "\n",
        "- Aprender como reaccionar a un movimiento que hace nuestro oponente.\n",
        "- Evaluar que tan buena es nuestra posición para ganar el juego.\n",
        "- Predecir que hará nuestro oponente en la siguiente jugada y usar esto para decidir nuestro movimienot actual.\n",
        "- Entender como jugarían los jugadores en general en una situación en la que nos encontramos en el momento.\n",
        "\n",
        "De hecho, los agentes de aprendizaje reforzado pueden aprender a hacer cosas similares. En particular, un agente está compuesto de múltiples funciones y modelos para asistir su toma de decisión. Existen tres componentes que los agentes pueden tener: \n",
        "- Una **política**\n",
        "- Una **función de valor**\n",
        "- Un **modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjvLYZEISzHN",
        "colab_type": "text"
      },
      "source": [
        "#### La *política* del *agente*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYZ4piW-TYbT",
        "colab_type": "text"
      },
      "source": [
        "Una **política** es un algoritmo o un conjunto de reglas que describe como un agente toma sus decisiones. Un ejemplo de una, puede ser la estrategia que un inversionista usa para invertir en sus acciones, donde este compra una acción cuando los precios bajan y la vende cuando los precios suben.\n",
        "\n",
        "Formalmente, una política es una función, denotada por $\\pi$ que mapea un estado $s_t$ a una acción $a_t$:\n",
        "\n",
        "$\\pi(s_t)=a_t$\n",
        "\n",
        "Lo cuál significa que el agente decide su acción tomando en cuenta su estado actual. Esta función puede representar muchas cosas, en la medida en que reciba un estado como input y una acción como output, sea una tabla, un grafo, o un clasificador de machine learning.\n",
        "\n",
        "Siendo estrictos en la manera que definimos la política, esta resulta ser **determinística** y no en todas las aplicaciones es así. Es por esto que aparece una manera más general de definir la política y es de manera **estocástica** donde esta tiene como output una distribución de probabilidad sobre el conjunto de acciones posibles en un estado:\n",
        "\n",
        "$\\pi(a_t|s_t)=P(a_t|s_t)$\n",
        "\n",
        "donde $\\pi(a_t| s_t)$ es un vector de probabilidades normalizado sobre el conjunto de posibles acciones dado un estado. Aplicando al ejemplo de Atari este concepto de política, veamos la siguiente imagen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3oLIem3lDQO",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://learning.oreilly.com/library/view/python-reinforcement-learning/9781788991612/assets/0ce55cbe-0718-4ad8-a8f7-f45d19212236.png_)\n",
        "\n",
        "Imagen: Como la política mapea el estado del juego (la pantalla) a las acciones (probabilidades)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5aYMI9IS-uB",
        "colab_type": "text"
      },
      "source": [
        "#### La *función de valor* del agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlRjH-jBmC0l",
        "colab_type": "text"
      },
      "source": [
        "El segundo componente que un agente puede tener es la **función de valor**. Como se mencionó en la introducción, es útil evaluar la posición, si es buena o mala, en un estado dado. En una partida de ajedrez, a un jugador le gustaría saber la verosimilitud de que va a ganar dado un estado del tablero. Un agente navegando un laberinto le gustaría saber que tan cerca está de llegar a su destino. La idea es que la función de valor quiere satisfacer los propósitos anteriores, ya que *predice el valor esperado de recompensa que un agente recibiría en un estado dado*. En otras palabras, mide que tan deseable es para el agente, su estado actual.\n",
        "\n",
        "Formalmente, la función de valor toma un estado y una política como input y retorna un valor escalar representando la esperanza de la recompensa acumulada:\n",
        "\n",
        "$v(s,\\pi)= E[R|s,\\pi]=E[r_0+\\gamma^1r_1+\\cdots+\\gamma^tr_t|s,\\pi]$\n",
        "\n",
        "Ahora, ¿De qué manera esta función ayuda al agente a desempeñar una tarea bien, a parte de informar sobre que tan deseable es estar en un estado dado? Como veremos luego, las funciones de valor juegan un papel importante en predecir que tan bueno será el tomar una serie de acciones antes de que sean tomadas por el agente. Esto es similar a que los jugadores de ajedrez se imaginen qué tan bien una secuencia de acciones futuras funcionará para mejorar sus posibilidades de ganar. Para esto, el agente también necesita tener un entendimiento de como el ambiente opera. Y este entendimiento, resulta en el tercer componente de un agente que es, **el modelo**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UspD7gneTECy",
        "colab_type": "text"
      },
      "source": [
        "#### El *modelo* del agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmdifE3wsBRV",
        "colab_type": "text"
      },
      "source": [
        "Anteriormente, se discutió como el agente no conoce completamente el ambiente. En otras palabras, el agente usualmente no tiene una idea de como se ve internamente el algoritmo completo del ambiente.\n",
        "\n",
        "El agente necesita interactuar con el ambiente para ganar información y aprender como maximizar el valor esperado de la recompensar acumulada. Sin embargo, es posible para el agente tener una réplica interna, o un modelo, de el ambiente. El agente puede usar el modelo para predecir de qué manera el ambiente reaccionaría a alguna acción en un estado dado. Un modelo en el mercado de las acciones, por ejemplo, tiene la tarea de predecir como los precios serán en un futuro. Si el modelo es preciso, el agente puede entonces usar su función de valor para evaluar que tan deseable es el futuro. \n",
        "\n",
        "Formalmente, el modelo puede ser representado por una función $M$ que calcula la probabilidad del siguiente estado dado el estado actual y un acción: (MONTENEGRO?)\n",
        "\n",
        "$M(a_t,s_t)=P(s_{t+1}|s_t,a_t)$\n",
        "\n",
        "En otro escenarios, el modelo del ambiente puede ser usado para enumerar posibles estados futuros. Esto es común en juegos basados por turno, tales como el ajedrez o triki, donde las reglas y alcance de las posibles acciones están claramente definidas. Normalmente se hacen árboles de para ilustrar estas posibles sucesiones de acciones y estados de juegos basados en turno:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZGxlsTVytR1",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://learning.oreilly.com/library/view/python-reinforcement-learning/9781788991612/assets/873f8b59-9dd2-4fc6-830a-9c5fd62198f9.png_)\n",
        "\n",
        "Imagen: Un modelo usando su valor de función para evaluar sus posibles movimientos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMiWAmtx0Vxw",
        "colab_type": "text"
      },
      "source": [
        "En el ejemplo anterior del juego triki, $M(s_t,a_t)$ denota los posibles estados que al tomar una acción $a_t$ (representada en la imagen como un círculo azul con rayas dentro) podría tomar en un estado dado $s_t$. Más aún, podemos calcular el valor de cada estado usando el valor de la *función de valor*. Los estados intermedios o de abajo en la imagen llevarían a obtener un valor alto ya que el agente estaría a un paso de ganar, caso contrario al caso de arriba ya que aquí se debe preocupar de que el oponente gane.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XeeF2w-4ruc",
        "colab_type": "text"
      },
      "source": [
        "#### Resumen de función"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFkry3Xl41K7",
        "colab_type": "text"
      },
      "source": [
        "Función | Descripción | Output\n",
        "--- | --- | ---\n",
        "Política | Algoritmo/función que retorna decisiones que el agente toma. | Un escalar único (decisión) (política determinística) o un vector de probabilidades sobre posibles acciones (política estocástica)\n",
        "Función de Valor |  Función que describe que tan bueno o malo es un estado dado. | Un valor escalar que representa el valor esperado de la recompensa acumulada.\n",
        "Modelo | Una representación del ambiente que tiene el agente, que predice como reaccionará el ambiente a las acciones que el agente realiza.| La probabilidad del próximo estado dados una acción y el estado actual, o una enumeración de posibles estados dadas las reglas del ambiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfYRibxwAi--",
        "colab_type": "text"
      },
      "source": [
        "## Infrastructura y Dependecias "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S0W8ek8Ax_5",
        "colab_type": "text"
      },
      "source": [
        "## Aprendizaje de Máquina en Acción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R37MbKayBAqC",
        "colab_type": "text"
      },
      "source": [
        "## "
      ]
    }
  ]
}